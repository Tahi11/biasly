{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eunbeejang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# For Data Preparation\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "# For Word Embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import logging\n",
    "\n",
    "# For LSTM Model\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Accessing the data\\nbias_data.value\\nbias_data[rows:cols]\\n\\n# Basic Stats about the data\\nbias_data.describe()\\nbias_data.count\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Processing\n",
    "\n",
    "'''\n",
    "#Read File\n",
    "with open('FILE NAME', 'r') as f:\n",
    "    bias_data = f.read()\n",
    "'''\n",
    "\n",
    "# Names for the columns (Header)\n",
    "cols = ['sentence', 'bias', 'sex', 'age', 'occupation', 'citizenship']\n",
    "\n",
    "# Load the data from a CSV (returns type(dataframe))\n",
    "bias_data = pd.read_csv('./scrapeBIG.csv', sep=',',  encoding='latin-1') # encoding='latin-1', names=cols, header=None,\n",
    "print(\"Data loaded\")\n",
    "\n",
    "\n",
    "# Randomize data\n",
    "bias_data = bias_data.reindex(np.random.permutation(bias_data.index))\n",
    "\n",
    "# Replace NAN with Zero(0)\n",
    "bias_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Accessing the data\n",
    "bias_data.value\n",
    "bias_data[rows:cols]\n",
    "\n",
    "# Basic Stats about the data\n",
    "bias_data.describe()\n",
    "bias_data.count\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Citizenship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>The case against TCS was filed in 2015 by Stev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The program aims to attract researchers from a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Says program head David Barrett: \"So many of o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>\"Eventually,\" he says, \"men will start going b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>I found that these menÕs relationships with ot...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Even when women and men behave in leaderly way...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>I felt no pressure and I wasn't trying to prov...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>If you shared the biological genitals of a wom...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>That's not to say that there aren't times when...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>The question I wanted to answer was: Did revie...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  Bias  Sex  Age  \\\n",
       "74   The case against TCS was filed in 2015 by Stev...   0.0  0.0  0.0   \n",
       "110  The program aims to attract researchers from a...   0.0  0.0  0.0   \n",
       "487  Says program head David Barrett: \"So many of o...   0.0  0.0  0.0   \n",
       "489  \"Eventually,\" he says, \"men will start going b...   0.0  0.0  0.0   \n",
       "394  I found that these menÕs relationships with ot...   0.0  0.0  0.0   \n",
       "378  Even when women and men behave in leaderly way...   0.0  0.0  0.0   \n",
       "694  I felt no pressure and I wasn't trying to prov...   0.0  0.0  0.0   \n",
       "246  If you shared the biological genitals of a wom...   0.0  0.0  0.0   \n",
       "264  That's not to say that there aren't times when...   0.0  0.0  0.0   \n",
       "123  The question I wanted to answer was: Did revie...   0.0  0.0  0.0   \n",
       "\n",
       "     Occupation  Citizenship  \n",
       "74          0.0          0.0  \n",
       "110         0.0          0.0  \n",
       "487         0.0          0.0  \n",
       "489         0.0          0.0  \n",
       "394         0.0          0.0  \n",
       "378         0.0          0.0  \n",
       "694         0.0          0.0  \n",
       "246         0.0          0.0  \n",
       "264         0.0          0.0  \n",
       "123         0.0          0.0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "bias_data[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = bias_data['Sentence']\n",
    "label = bias_data['Bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine walking down the street and seeing advertising screens change their content based on how you walk, how you talk, or even the shape of your chest.\n"
     ]
    }
   ],
   "source": [
    "#print(sentence)\n",
    "#print(label)\n",
    "print(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a sentence into a list of words\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # 1. Remove non-letters\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = sentence_text.lower().split()\n",
    "    # 3. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagine',\n",
       " 'walking',\n",
       " 'down',\n",
       " 'the',\n",
       " 'street',\n",
       " 'and',\n",
       " 'seeing',\n",
       " 'advertising',\n",
       " 'screens',\n",
       " 'change',\n",
       " 'their',\n",
       " 'content',\n",
       " 'based',\n",
       " 'on',\n",
       " 'how',\n",
       " 'you',\n",
       " 'walk',\n",
       " 'how',\n",
       " 'you',\n",
       " 'talk',\n",
       " 'or',\n",
       " 'even',\n",
       " 'the',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'your',\n",
       " 'chest']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_wordlist(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of sentences where each sentence is a list of words\n",
    "def data_to_sentences(data, tokenizer, remove_stopwords=False ):\n",
    "    try:\n",
    "        # 1. Use the NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(data.strip())\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        # 3. Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        len(sentences)\n",
    "        #print('GOOD')\n",
    "        return sentences\n",
    "    except:\n",
    "        print('ERROR :', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 895 sentences in the corpus of bias_data.\n"
     ]
    }
   ],
   "source": [
    "#bias_sentences = bias_data['sentence'].tolist()\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in range(0,len(sentence)):\n",
    "    #try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "    data = sentence[i] #.replace(\"/.\", '')\n",
    "        # Now apply functions\n",
    "    sentences += data_to_sentences(data, tokenizer)\n",
    "    #except:\n",
    "    #    print('Nonono! ERROR in ... creating a lit of sentences' ')\n",
    "\n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in the corpus of bias_data.\")\n",
    "              \n",
    "# Accessing each sentences\n",
    "              \n",
    "# sentences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels = bias_data['bias'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3825, size=100, alpha=0.025)\n",
      "[-0.10636672 -0.14265263 -0.02078844 -0.0232397  -0.11916558 -0.01144145\n",
      " -0.17669465 -0.04775187 -0.02393734  0.08522997  0.03947792  0.04068775\n",
      "  0.14863844 -0.04276646  0.056711   -0.00946584 -0.07331948  0.01676305\n",
      "  0.22920586  0.11169035 -0.09725992  0.05375832  0.02720733  0.06562116\n",
      " -0.08984692 -0.14993925  0.03146018 -0.05324413 -0.22514331 -0.10860506\n",
      " -0.04654963 -0.15635628  0.28395778 -0.04567298 -0.07397024  0.00344704\n",
      "  0.18910787 -0.07491151 -0.01727845 -0.00496828  0.02231907  0.06447957\n",
      "  0.08648467 -0.0159786  -0.07405483  0.00616721  0.0440251  -0.19083989\n",
      "  0.0012248  -0.05227284 -0.17266096  0.12702642 -0.01785207  0.10279123\n",
      " -0.02607189 -0.05348418 -0.03739962 -0.04781531 -0.10755043 -0.14032726\n",
      "  0.05204801 -0.14130357 -0.02954207 -0.11977896 -0.01943543  0.05340746\n",
      "  0.10656927  0.06546345  0.08271471 -0.02562047  0.00571597  0.02433201\n",
      "  0.19804275 -0.1523862  -0.04308036  0.00227266 -0.07295103  0.15554589\n",
      "  0.09296194  0.0057441  -0.23692738  0.05693604 -0.18326135 -0.07464931\n",
      "  0.06195853  0.06886388 -0.03906531  0.09582384 -0.12373053 -0.07857344\n",
      " -0.01514772  0.05607762 -0.09136649 -0.14177297  0.11268213 -0.01402271\n",
      " -0.00176392  0.01183616 -0.06205987 -0.17346673]\n",
      "[-0.11181911 -0.12426946 -0.04668496 -0.07924275 -0.0762506  -0.00347624\n",
      " -0.20052323 -0.03632051  0.00895623  0.03652689  0.01712092 -0.01745088\n",
      "  0.20336886 -0.08701386  0.01105385 -0.06061172 -0.05945717  0.00440394\n",
      "  0.23736919  0.08222621 -0.11952974  0.06616063 -0.0162661   0.04681183\n",
      " -0.02725099 -0.15819062  0.05527201 -0.10544278 -0.18792568 -0.05126287\n",
      " -0.04733444 -0.09848071  0.2188613  -0.00568454 -0.10656343  0.01988625\n",
      "  0.20116886 -0.02292757  0.02740242  0.0427596   0.01640006  0.04942708\n",
      "  0.09340169  0.03173338 -0.1103354  -0.00234483  0.01371227 -0.19971794\n",
      " -0.01297426 -0.10679215 -0.12056775  0.1592916   0.00065661  0.09396766\n",
      "  0.01192756 -0.0487099  -0.08150762 -0.01432444 -0.07371669 -0.1932475\n",
      "  0.09210084 -0.10492688  0.02580318 -0.09409821 -0.0570157   0.0603276\n",
      "  0.11784327  0.00925636  0.06048637 -0.05983415 -0.01238819  0.02586433\n",
      "  0.18823637 -0.10720088 -0.06419943 -0.07488015 -0.03086146  0.18970779\n",
      "  0.09299436  0.00848822 -0.22875068  0.0077459  -0.17609823 -0.06433548\n",
      "  0.08819724  0.08275845 -0.06760964  0.13448481 -0.11594834 -0.10867509\n",
      "  0.01924     0.03992666 -0.1243518  -0.11959445  0.17507441  0.00050211\n",
      "  0.04721653 -0.02146409 -0.02777559 -0.16527545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# save model\\nmodel.save('model.bin')\\n\\n# load model\\nnew_model = Word2Vec.load('model.bin')\\nprint(new_model)\\n\\n# access vector for one word\\nprint(model['sentence'])\\n\\n\\n\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Word Vectors\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Calling init_sims will make the model will be better for memory if we don't want to train the model over and over again\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# Summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "\n",
    "print(model['case'])\n",
    "print(model['program'])\n",
    "\"\"\"\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Test (tensor flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.60788 \n",
      "prediction:  [[3 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  llllll \n",
      "==============\n",
      "1 loss: 1.51026 \n",
      "prediction:  [[3 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  llllll \n",
      "==============\n",
      "2 loss: 1.4327 \n",
      "prediction:  [[3 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  llllll \n",
      "==============\n",
      "3 loss: 1.34895 \n",
      "prediction:  [[3 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  llllll \n",
      "==============\n",
      "4 loss: 1.25513 \n",
      "prediction:  [[1 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  illlll \n",
      "==============\n",
      "5 loss: 1.14044 \n",
      "prediction:  [[1 3 3 3 3 3]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  illlll \n",
      "==============\n",
      "6 loss: 1.01676 \n",
      "prediction:  [[1 3 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ilello \n",
      "==============\n",
      "7 loss: 0.896927 \n",
      "prediction:  [[1 3 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ilello \n",
      "==============\n",
      "8 loss: 0.769525 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "9 loss: 0.655007 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "10 loss: 0.542758 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "11 loss: 0.428471 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "12 loss: 0.334515 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "13 loss: 0.247502 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "14 loss: 0.181771 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "15 loss: 0.13268 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "16 loss: 0.0943341 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "17 loss: 0.0664921 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "18 loss: 0.0477194 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n",
      "19 loss: 0.0350963 \n",
      "prediction:  [[1 0 2 3 3 4]] \n",
      "true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\n",
      "Prediction str:  ihello \n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# RNN/LSTM Model\n",
    "\n",
    "# One cell RNN input_dim(4) -> output_dim(2)\n",
    "# sequnce_length \n",
    "num_classes = 5\n",
    "learning_rate = 0.1\n",
    "hidden_size = 5 # size of output from the LSTM\n",
    "input_dim = 5 # one hot size\n",
    "sequence_length = 6 # |hihello| == 6\n",
    "batch_size = 1 # num of words to be inputted each training\n",
    "\n",
    "\n",
    "\n",
    "# Data Creation\n",
    "# idx2char: dictionary\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
    "\n",
    "\"\"\"\n",
    "# One hot Encoding\n",
    "h = [1, 0, 0, 0, 0] # 0:h\n",
    "i = [0, 1, 0, 0, 0] # 1:i\n",
    "e = [0, 0, 1, 0, 0] # 2:e\n",
    "l = [0, 0, 0, 1, 0] # 3:l\n",
    "o = [0, 0, 0, 0, 1] # 4:0\n",
    "\n",
    "# shape of x_data : (batch size, sequence length, one hot vocab vector size)\n",
    "x_data = np.array([[h,e,l,l,o],[e,o,l,l,l],[l,l,e,e,l]], dtype=np.float32)\n",
    "print(\"x_data.shape = \", x_data.shape)\n",
    "pp.pprint(x_data)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True, reuse=tf.AUTO_REUSE)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# shape of output : (batch size, sequence length, hidden size)\n",
    "print(\"output.shape = \", outputs.shape)\n",
    "pp.pprint(outputs.eval())\n",
    "print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim]) # X one-hot, None: Batch size\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "\n",
    "# Basic LSTM\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True, reuse=tf.AUTO_REUSE)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32) # Initial State is always ZERO\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Sequence_Loss\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "\n",
    "# Teach RNN 'ihello'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"\\nprediction: \", result, \"\\ntrue Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\nPrediction str: \", ''.join(result_str), \"\\n==============\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM MODEL for Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
